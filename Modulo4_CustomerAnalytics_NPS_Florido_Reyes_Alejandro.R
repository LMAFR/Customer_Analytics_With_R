# Autor: Alejandro Flordo Reyes.

#Practica Matricas de Satisfaccion del Consumidor - NPS
#NPS - Net Promoter Score - Score promedio de recomendacion
#En R existe una libreria llamada NPS para calcularlo automaticamente
#Deber치n trabajar el calculo NPS sobre un dataset y analizar correlaciones con cluster de RFM


#PASO 1: Deben cargar las librerias tidyverse, lubridate, caret, corrplot y NPS

library(tidyverse)
library(lubridate)
library(caret)
library(corrplot)
library(NPS)

#PASO 2: Generen un dataset llamado ventas2020 cargando el archivo "NPS_T1.csv" eliminando primera columna
#Eliminen aquellos registros que figuren con un NPS NA utilizando filter
#Modifiquen la columna nps a columna numerica utilizando mutate y as.numeric
#Ayuda: al utilizar select si escriben select(-1) entonces se seleccionan todas las columnas excepto la primera

# setwd("C:/Documentos y otros/EAE/Asignaturas y Master/Customer Analytics/ModuloIV_PracticaNPS")

# Primero guardamos el dataframe con los datos de la tabla, salvo la primera columna. Para ello,
# previamente he ojeado el archivo .csv con excel.

ventas2020 <- read.csv("NPS_T1.csv", header = T, sep =',') %>% 
  select(-1)
head(ventas2020) # Comprobamos que nos la ha eliminado.
table(is.na(ventas2020)) # Hay 2951 NA's en el dataset.

#* Vemos lo que nos est치 mostrando y comprobamos que es lo que queremos (1 columna 
#* de fechas, 1 de caracteres y 3 de integers).
#* 
#* Comprobamos que los tipos asignados a cada columna son correctos. 

str(ventas2020)   

# A la columna de fechas le ha dado tipo character, vamos a arreglarlo por si nos pidieran 
# trabajar con ella.

ventas2020$fecha_compra <- as.Date(ventas2020$fecha_compra) # Asignamos el tipo Date a la columna de fechas.
str(ventas2020) # Ahora parece que todo est치 correcto (los tipos de columna son los que cabr칤a esperar).

ventas2020 <- ventas2020 %>% 
  filter(!is.na(nps)) %>% 
  mutate(nps = as.numeric(nps))
View(ventas2020)
str(ventas2020)

table(is.na(ventas2020)) # Comprobamos si hay m硬 NA's en el dataset. Vemos que no.

#Calculen el NPS agrupado por cada tienda, utilizando la funci칩n nps del paquete NPS dentro de la funcion summarise
#쯖ual es la tienda con mejor performance?

nps_tiendas <- ventas2020 %>%
  group_by(store) %>%
  summarise(NPS = nps(nps))
nps_tiendas # Resultados:

# store      NPS
# <chr>    <dbl>
# 1 tienda_1 0.143
# 2 tienda_2 0.135
# 3 tienda_3 0.182
# 4 tienda_4 0.192
# 5 tienda_5 0.183

#* Dado que 1 ser칤a el mejor caso y -1 el peor, la tienda con mejor performance es la 4.

#Realizaremos un analisis entre los meses del a침o y el NPS para cada tienda
#para ello deben crear una columna mes en el dataframe de ventas2020 
#y agrupar tanto por mes como por store y calcular el nps en un nuevo data frame

ventas2020$mes <- month(ventas2020$fecha_compra) # Tambi칠n podr칤a haberla creado con mutate.
View(ventas2020)

nps_mes_tienda <- ventas2020 %>%
  group_by(store,mes) %>% 
  summarise(NPS = nps(nps))
nps_mes_tienda # Solo ha llegado hasta el mes 7 (Julio) as칤 que vamos a comprobar si no
               # hay m치s meses incluidos en las fechas de las tablas.

max(ventas2020$mes) # En efecto, los datos solo contienen fechas hasta Julio.

#visualizamos la comparaci칩n de NPS de cada tienda para cada mes
#utilicen gr치fico de scatter (geom_point) y den color a los puntos con
#columna store

ggplot(nps_mes_tienda) +
  geom_point(aes(x = mes, y = NPS, colour = store)) +
  labs(title = 'NPS de cada tienda en cada mes', x = 'Mes', y = 'NPS') +
  facet_wrap(~store, ncol = 5) +
  theme_bw() # Esto es lo que se nos pide con un poco de decoraci칩n (t칤tulo, tema, etc.) y dividido por ventanas, que
             # es algo que se me ha recomendado en el feedback de la pr치ctica de CRM.

# Yo creo que quedar칤a m치s claro de esta otra forma: (esto lo escrib칤 antes del feedback mencionado, pero creo que merece la pena dejarlo)

ggplot(nps_mes_tienda) +
  geom_point(aes(x = mes, y = NPS, colour = store)) +
  geom_line(aes(x = mes, y = NPS, colour = store)) +
  labs(title = 'NPS de cada tienda en cada mes', x = 'Mes', y = 'NPS') +
  facet_wrap(~store, ncol = 5) +
  theme_bw() # Yo creo que esto es algo m치s claro, ya que los colores no est치n todos en la misma gr치fica y las l칤neas ayudan
             # a tener una idea de la variaci칩n del NPS que ha tenido cada tienda, haciendo m치s clara la comparaci칩n entre 
             # tiendas para cada mes. Yo hubiera a침adido estas l칤neas al gr치fico anterior tambi칠n, solo por mostrar m치s clara-
             # mente cada conjunto de valores de NPS (creo que el color no ayuda mucho en este caso), pero la gr치fica anterior
             # he preferido presentarla simplemente como se ped칤a para que supieran que sab칤a dibujarla.

#* Podemos observar que en el 칰ltimo mes la tienda 3 es la que ha obtenido mejores resultados, no la 3 (que era la que
#* ten칤a mejor NPS de las tiendas). Por otro lado las otras dos con mayor NPS son la 4 y la 5 y vemos que, en general, 
#* obtienen valores m치s altos que los de las tiendas 1 y 2, tanto en general (como era de esperar) como en ese 칰ltimo mes.


#Desarrollar el c치lculo de RFM para cada comprador en otro dataframe 
#sin olvidar de modificar la columna de  fecha para que R la reconozca como tal utilizando as.Date
#Generen 5 clusters a traves de kmean para identificar segmentos de consumidores
#pueden utilizar de referencia el script visto en el modulo II

#* Ya he convertido la columna de fechas al tipo fecha antes (l칤nea 41), as칤 que vamos directos al c치lculo 
#* del RFM:

fecha_max = max(ventas2020$fecha_compra) # Voy a calcular esto para calcular la recencia, a continuaci칩n explico por qu칠.
str(fecha_max) # Aunque es obvio, comprobemos que es tipo fecha.

#* Los c치lculos del valor monetario y la frecuencia son valores constantes para cada miembro (como es l칩gico), ya que se
#* calculan directamente a partir del dataframe. Sin embargo, el c치lculo de la recencia tal y como lo tenemos en la pr치ctica
#* hecha en clase se hace con as.Date(Sys.Date()) y creo que eso no es correcto por varios motivos. En primer lugar, hacer eso
#* implica que cada d칤a que corramos el c칩digo vamos a tener diferentes valores de recencia, por lo que la segmentaci칩n que 
#* vamos a hacer posteriormente en esta pr치ctica va a cambiar seg칰n el d칤a en que corramos el c칩digo (porque depende de este
#* par치metro) y eso har칤a imposible evaluar el an치lisis realizado, ya que los resultados que yo haya analizado pueden ser diferentes
#* a los que vea el profesor cuando corrija. En segundo lugar, si hacemos el c치lculo de recencia usando como uno de los t칠rminos el
#* d칤a actual, entonces estamos calculando la recencia con respecto a la actualidad, pero los datos del dataset son de hace 4 칩 5
#* meses los m치s actuales. En consecuencia, hacer el c치lculo de la recencia para la actualidad implicar칤a suponer que los clientes
#* no han vuelto a comprar desde entonces y eso es algo que no sabemos ni podemos suponer.
#* 
#* *Nota. Originalmente hice los c치lculos de la recencia con as.Date(Sys.Date()). Si fuera necesario, estos c치lculos se pueden comprobar
#* en el script que adjuntar칠 junto a este al entregar el trabajo.
#* 
#* Dicho esto, procedo a calcular la recencia con respecto al 칰ltimo d칤a en que se tomaron datos, que fue fecha_max (31/07/2020).

calculo_RFM <- ventas2020 %>% 
  group_by(id_member) %>% 
  summarise(Recency = as.numeric(fecha_max-max(fecha_compra)),
            Frequency=length(id_member),
            Monetary_value=sum(gasto))
head(calculo_RFM) # Visualicemos el resultado.

#* Ya tenemos el c치lculo del RFM por comprador, ahora vamos a generar los 5 clusters para 
#* identificar segmentos de consumidores.

set.seed(4321) #* Plantamos una semilla para que el resultado obtenido sea reproducible (esto es,
               #* que cuando los profesores corrij치is esto os salga lo mismo que me sali칩 a m칤,
               #* por poner un ejemplo relevante).

segmentacionRFM <- kmeans(scale(calculo_RFM[,2:4]), 5, nstart = 1) # Creamos un vector con 5
# categor칤as o clusters y un n칰mero de elementos igual al n칰mero de filas del dataframe a partir
# del cual se ha generado.

str(segmentacionRFM)

calculo_RFM$ScoreRFM <- as.factor(segmentacionRFM$cluster) # Almacenamos el nuevo vector 
                                                           # generado dentro del dataframe y en forma de factor.

head(calculo_RFM) 

# Para que quede m치s bonito de cara a equipos de marketing y perfiles no
# t칠cnicos en general, podemos cambiar los n칰meros de los clusters por 
# palabras.

calculo_RFM$Tipo_de_cliente <- NA
calculo_RFM$Tipo_de_cliente[calculo_RFM$ScoreRFM == 1] <- "inactivo"
calculo_RFM$Tipo_de_cliente[calculo_RFM$ScoreRFM == 2] <- "en riesgo"
calculo_RFM$Tipo_de_cliente[calculo_RFM$ScoreRFM == 3] <- "potenciales"
calculo_RFM$Tipo_de_cliente[calculo_RFM$ScoreRFM == 4] <- "leales"
calculo_RFM$Tipo_de_cliente[calculo_RFM$ScoreRFM == 5] <- "champions"

head(calculo_RFM)

#Calcular nps agrupando por segmento de consumidores en un nuevo data frame

#* En primer lugar, unimos el dataFrame calculo_RFM al original (ventas2020), de tal forma que
#* asignemos a cada miembro en dicho dataframe su correspondiente valor de cl칰ster:
 
ventas2020 = ventas2020 %>%
  left_join(calculo_RFM, by = c("id_member" = "id_member")) # S칠 que el by no es necesario en este caso, pero es para que se vea
                                                            # que s칠 usar la funci칩n y que s칠 lo que estoy haciendo.

#* Como las dimensiones de ambas tablas no son iguales, unimos la tabla calculo_RFM a la de partida, que solo implica que
#* los valores de recencia, frecuencia, valor monetario, ScoreRFM y Tipo_de_cliente se asignen en la tabla original a cada
#* id_member (recordemos que en la tabla original estas id se repiten, ya que no est치 agrupada por las mismas como lo est치
#* el dataset calculo_RFM).

View(ventas2020) # Comprobamos que se ha asignado cada Tipo de cliente del cluster a su correspondiente id_member 
                  # observando su valor en el dataframe para los 3 primeros id_members.

# Calculamos el nps:

nps_clusters = ventas2020 %>% 
  group_by(Tipo_de_cliente) %>% 
  summarise(NPS = nps(nps))

nps_clusters # Resultados:

# champions       0.156
# en riesgo       0.171
# inactivo        0.178
# leales          0.182
# potenciales     0.150

#* Nota importante: hemos obtenido los mismos resultados con la recencia calculada con la fecha actual (script adjunto) que
#* (como se ha hecho ahora) con la recencia calculada respecto a la 칰ltima fecha del dataset, lo que me tienta a decir que 
#* el clustering con kmeans no tiene en cuenta la recencia (aunque no s칠 si poner la mano en el fuego, habr칤a que terminar
#* de investigarlo indagando en su documentaci칩n).

#* Observamos que los leales son los que tienen mayor NPS, seguidos de cerca por los inactivos.

#Ahora realicen una correlacion entre NPS y  los segmentos de consumidores de RFM
#Existen mayor correlacion con aquellos consumidores que gastan mas dinero o menos dinero?

lcorr = lm(nps_clusters$NPS ~ c(1:5)) # Como la variable categ칩rica va del 1 al 5, la he representado con 
                              # un vector que contenga esos elementos para evitar problemas, ya que
                              # ambas variables deben ser cuantitativas para el an치lisis.
lcorr

a = lcorr$coefficients[2]
b = lcorr$coefficients[1]

# Pendiente: 0.0000505
# Ord. en el origen: 0.167

# Vemos que la pendiente es bastante peque침a con respecto a la ordenada en el origen.
# Por si acaso, hag치mosle un test de correlaci칩n lineal:

cor.test(nps_clusters$NPS,c(1:5)) # Vemos que p_value = 0.9927 > 0.05, luego no hay correlaci칩n lineal (hip칩tesis nula).

# Sin embargo, en los resultados que hab칤amos obtenido, quitando los champions, parec칤a que el NPS aumentaba conforme
# los consumidores ten칤an un RFM mayor. Solo por si acaso, dibujemos los puntos y la recta de ajuste:

# plot(c(1:5),nps_clusters$NPS)
# abline(lm(nps_clusters$NPS ~ c(1:5))) # Esto ser칤a un plot cutre.

ggplot(nps_clusters) +
  geom_point(aes(x=c(1:5), y = NPS)) +
  stat_function(fun=function(x) a*x+b) +
  xlim(0,5) +
  labs(title = 'NPS para cada Tipo de cliente', x = 'Tipo de cliente (n췈)', y = 'NPS') +
  theme_bw()

# Recordemos que:

# 1 <- "inactivo"
# 2 <- "en riesgo"
# 3 <- "potenciales"
# 4 <- "leales"
# 5 <- "champions"

# Podemos ver que la recta no se corresponde con los puntos, ya que para los champions tenemos un valor
# an칩malo (no se adapta para nada a la progresi칩n que vemos para el resto de tipos de clientes, que si
# parece m치s lineal). Por lo tanto, hagamos un peque침o c치lculo sin tener en cuenta los champions:

cor.test(nps_clusters$NPS[1:4],c(1:4)) # Ahora p_value = 0.035 < 0.05; por lo tanto, s칤 que hay correlaci칩n lineal.
                                       # Tambi칠n podemos observar un coeficiente de correlaci칩n lineal bastante elevado 
                                       # (de 0.96). 

# En consecuencia, concluimos que hay mayor correlaci칩n entre los consumidores con una menor RFM_Score que, a priori, son
# los que gastan menos, ya que se entiende que tienen valores de frecuencia y valor monetario (que est치n directamente ligados 
# al gasto, ya sea por hacer muchos pedidos o por la cantidad total gastada en las tiendas) m치s bajos que los de score 
# elevada. Estos 칰ltimos presentan una correlaci칩n m치s rara, ya que de potenciales a leales parece continuar la correlaci칩n 
# lineal, pero luego los champions rompen la correlaci칩n al comportarse de manera muy distinta a los otros segmentos.

lcorr2 = lm(nps_clusters$NPS[1:4] ~ c(1:4))

a2 = lcorr2$coefficients[2]
b2 = lcorr2$coefficients[1]

# plot(c(1:4),nps_clusters$NPS[1:4])
# abline(lm(nps_clusters$NPS[1:4] ~ c(1:4))) # Dejo aqu칤 un dibujo esquem치tico para que se vea el ajuste sin los champions.

ggplot(nps_clusters[1:4,1:2]) +
  geom_point(aes(x=c(1:4), y = NPS)) +
  stat_function(fun=function(x) a2*x+b2) +
  xlim(0,4) +
  labs(title = 'NPS para cada Tipo de cliente', x = 'Tipo de cliente (n췈)', y = 'NPS') +
  theme_bw()

#Que sucede si realizamos un promedio de NPS por cada segmentos para cada tienda?
#los segmentos punt칰an muy diferente a cada tienda? Observamos algun patron?

nps_segm_tienda = ventas2020 %>% 
  group_by(store, Tipo_de_cliente) %>% 
  summarise(Mean_nps = mean(nps))

head(nps_segm_tienda)

# Para visualizar patrones y si punt칰an m치s o menos diferente, lo mejor es hacer gr치ficas:

ggplot(nps_segm_tienda) +
  geom_point(aes(x = rep(c(1:5), 5), y = Mean_nps, colour = store)) +
  geom_line(aes(x = rep(c(1:5), 5), y = Mean_nps, colour = store)) +
  labs(title = 'Puntuaci칩n promedio de cada tipo de cliente para cada tienda', x = 'Tipo de cliente (n췈)', y = 'Puntuaci칩n promedio') +
  theme_bw() # Gr치ficas juntas.

  ggplot(nps_segm_tienda) +
  geom_point(aes(x = rep(c(1:5), 5), y = Mean_nps, colour = store)) +
  geom_line(aes(x = rep(c(1:5), 5), y = Mean_nps, colour = store)) +
  labs(title = 'Puntuaci칩n promedio de cada tipo de cliente para cada tienda', x = 'Tipo de cliente (n췈)', y = 'Puntuaci칩n promedio') +
  facet_wrap(~store, ncol = 5) +
  theme_bw() # Gr치ficas por separado.
  
#* Nota. Ambas variables deben ser cuantitativas para llevar a cabo el gr치fico de l칤neas, as칤 que no he puesto el tipo de cliente
#* en forma categ칩rica en el eje X, sino su equivalente num칠rico, que recordemos que es:
#* 
  # 1 <- "inactivo"
  # 2 <- "en riesgo"
  # 3 <- "potenciales"
  # 4 <- "leales"
  # 5 <- "champions"
  
#* Vemos que todos los segmentos punt칰an entre 7.5 y 8.05 aproximadamente para todas las tiendas, as칤 que lo primero que hay que
#* decir es que no punt칰an muy diferente en general. Ahora bien, si entramos en detalle, podemos observar casos individuales en 
#* los que s칤 se punt칰a bastante diferente seg칰n la tienda; por ejemplo,  los potenciales (3) punt칰an bastante m치s bajo en la
#* tienda 1 que en las otras.
#* 
#* Resumi칠ndolos, hay que destacar que la tienda 2 est치 obteniendo notas muy bajas para los clientes tipo champion, la 1 est치 
#* obteniendo la peor puntuaci칩n promedio para los leales y los potenciales y que la 5 est치 obteniendo notas bastante altas 
#* para los champions.
#* 
#* En cuanto a patrones, en la gr치fica con todas las curvas podemos observar cierta tendencia a mantener las puntuaciones entre
#* un 7.75 y un 7.95, que es el intervalo donde est치n contenidos la mayor parte de los valores. En cuanto a la representaci칩n
#* de gr치ficas separadas, lo 칰nico destacable es que en 4 de las 5 tiendas, los leales punt칰an m치s alto que los potenciales, lo
#* que, aunque es un poco rebuscado, es el 칰nico patr칩n relativamente fiable que he podido extraer de esta segunda 
#* representaci칩n ("los leales tienden a puntuar m치s alto que los potenciales").

#Que sucede si correlacionamos frecuencia de compra de los 172 ids con el NPS? 
#Los consumidores que tienen mayor frecuencia de compra punt칰an mas y mejor?

nps_score2 <- ventas2020 %>% 
  group_by(id_member) %>% 
  summarise(Frequency=length(id_member), NPS = nps(nps)); 

# Le a침adimos el c치lculo de la frecuencia, ya realizado antes en calculo_RFM para que el dataframe contenga ambas columnas:

View(nps_score2)

cor.test(nps_score2$Frequency, nps_score2$NPS) #* p_value = 0.2751 > 0.05. Se rechaza la hip칩tesis nula y, por lo tanto,
                                               #* se observa cierta correlaci칩n lineal entre ambas variables. Sin embargo,
                                               #* el coeficiente de correlaci칩n es peque침o (-0.084) y negativo (luego la 
                                               #* pendiente es negativa y, en consecuencia, tenemos una relaci칩n de 
                                               #* proporcionalidad inversa).

# Calculemos los par치metros de la recta de ajuste:

lcorr3 <- lm(nps_score2$NPS~nps_score2$Frequency) 
summary(lcorr3) #* Como era de esperar, r^2 es muy peque침o: 0.0070 (aproximadamente).

a3 = lcorr3$coefficients[2] # Pendiente: -0.00142  (aproximadamente).
b3 = lcorr3$coefficients[1] # Ordenada en el origen: 0.247 (aproximadamente). 

# plot(nps_score2$Frequency,nps_score2$NPS) 
# abline(lcorr3) 

ggplot(nps_score2) +
  geom_point(aes(x = Frequency, y = NPS)) + # Dibujamos el gr치fico de dispersi칩n asociado.
  stat_function(fun=function(x) a3*x+b3) + # Dibujamos la recta de regresi칩n.
  xlim(min(nps_score2$Frequency),max(nps_score2$Frequency)) +
  labs(title = 'NPS en funci칩n de la Frecuencia de compra de cada cliente', x = 'Frecuencia', y = 'NPS') +
  theme_bw()

#* Obtenemos que, a mayor frecuencia, menor nps y, por lo tanto, a mayor frecuencia de compra,
#* los clientes punt칰an peor. En cualquier caso, la correlaci칩n es MUY mala y, por lo tanto, las 
#* predicciones que nos proporciona la recta de regresi칩n van a carecer de precisi칩n (no son fiables).
#* Esto 칰ltimo se puede ver reflejado en la desviaci칩n t칤pica de la pendiente, cuyo valor (obtenido con
#* la funci칩n summary) era de, aproximadamente, 0.0013 (enorme, teniendo en cuenta que la pendiente vale 
#* -0.00142, aproximadamente).
#* 
#* En conclusi칩n: aparentemente, los consumidores con mayor frecuencia de compra punt칰an peor a la tienda,
#* pero no podemos estar seguros de que este resultado sea fiable. Una posible soluci칩n ser칤a estudiar una 
#* muestra mayor y ver si se obtienen resultados m치s fiables. 

#En l칤neas generales luego del an치lisis exploratorio, 쯣odriamos identificar tiendas que sobresalen
#por una buena o una mala performance en terminos de NPS?

  #* Al principio vimos que la tienda 4 era la que ten칤a mejor NPS si agrupamos por tienda y que 
  #* la 3 y la 5 no se alejaban mucho de su valor, por lo que la 1 y la 2 parec칤an ser las peores. Despu칠s,
  #* vimos en las gr치ficas que la tienda 3 ha sido la que mejor NPS ha obtenido el 칰ltimo mes del que
  #* tenemos datos y que, en efecto, junto a la 4 y la 5, obtiene mejores valores de NPS, en general, 
  #* que las tiendas 1 y 2. Por 칰ltimo, hemos visto que la tienda 2 obtiene una puntuaci칩n promedio especialmente mala 
  #* de sus champions, que la tienda 1 no se queda atr치s y tiene las peores notas en promedio para 
  #* leales y potenciales, y que la tienda 5 est치 obteniendo notas especialmente altas para los champions.
  #* 
  #* Por lo tanto, hemos identificado a las tiendas 3, 4 y 5 como las mejores (destacando cada una en 
  #* un aspecto relevante a nivel cuantitativo) y a las tiendas 1 y 2 como las peores (todo en t칠rminos 
  #* de NPS).

#Pueden utilizar los dataset NPS_T2.csv y NPS_T3.csv para seguir practicando
#Y realizando cruces de informacion


